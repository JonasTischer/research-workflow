\chapter{Related Work}
\label{ch:related-work}

\section{Transformer Architectures}

The transformer architecture was introduced by \citet{vaswani2017attention},
achieving state-of-the-art results on machine translation tasks.
The original model achieved 28.4 BLEU on the WMT 2014 English-to-German
translation benchmark.

% VERIFICATION EXAMPLE - Exact claim:
% ✅ "achieved 28.4 BLEU" - Verifiable fact from the paper

Self-attention mechanisms allow the model to attend to all positions
in the input sequence simultaneously, unlike recurrent architectures
which process tokens sequentially.

\section{Vision Transformers}

\citet{dosovitskiy2020image} demonstrated that transformers can achieve
competitive results on image classification when trained on large datasets.

% VERIFICATION EXAMPLE - Check for nuance:
% ⚠️ "when trained on large datasets" - Important qualifier!
% The paper notes ViT underperforms CNNs on smaller datasets.

% BAD CITATION (overclaiming):
% "ViT outperforms CNNs on all image tasks" - WRONG, paper doesn't claim this

Their Vision Transformer (ViT) model processes images as sequences of patches,
dividing each image into fixed-size patches (e.g., 16x16 pixels).

\section{Language Models}

Large language models have shown remarkable few-shot learning capabilities
\citep{brown2020language}. GPT-3 demonstrated that scaling model size
enables learning from just a few examples.

% VERIFICATION EXAMPLE - Meaning check:
% Ask: "Does the paper support 'remarkable few-shot learning'?"
% Check the actual few-shot results in the paper.

BERT \citep{devlin2018bert} introduced bidirectional pretraining,
which significantly improved performance on downstream NLP tasks.

\section{AI-Assisted Writing}

Recent work has explored using language models for academic writing assistance.
% TODO: Add relevant citations - find papers on:
% python src/web_search.py scholar "language models academic writing"

\section{Summary}

This chapter reviewed the key developments in transformer architectures
and their applications. The next chapter describes our methodology.
